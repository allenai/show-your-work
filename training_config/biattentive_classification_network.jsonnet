// GPU to use. Setting this to -1 will mean that we'll use the CPU.
local CUDA_DEVICE = std.parseInt(std.extVar("CUDA_DEVICE"));

// learning rate of overall model.
local LEARNING_RATE = std.extVar("LEARNING_RATE");

local BATCH_SIZE = std.parseInt(std.extVar("BATCH_SIZE"));

local EVALUATE_ON_TEST = std.parseInt(std.extVar("EVALUATE_ON_TEST")) == 1;

local NUM_EPOCHS = std.parseInt(std.extVar("NUM_EPOCHS"));

local GRAD_NORM = std.extVar("GRAD_NORM");

local EMBEDDINGS = std.split(std.extVar("EMBEDDINGS"), " ");
local FREEZE_EMBEDDINGS = std.split(std.extVar("FREEZE_EMBEDDINGS"), " ");



local GLOVE_FIELDS(trainable) = {
  "glove_indexer": {
    "tokens": {
      "type": "single_id",
    }
  },
  "glove_embedder": {
    "tokens": {
        "embedding_dim": 50,
        "trainable": trainable,
        "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.50d.txt.gz",
    }
  },
  "embedding_dim": 50
};


local ELMO_LSTM_FIELDS(trainable) = {
  "elmo_lstm_indexer": {
    "elmo": {
      "type": "elmo_characters",
    }
  },
  "elmo_lstm_embedder": {
    "elmo": {
      "options_file": "https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json",
      "weight_file": "https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5",
      "do_layer_norm": false,
      "dropout": 0.0,
      "requires_grad": trainable,
      "num_output_representations": if std.parseInt(std.extVar("USE_INTEGRATOR_OUTPUT_ELMO")) == 0 then 1 else 2
    }
  },
  "embedding_dim": 1024
};



local ELMO_LSTM_TRAINABLE = if std.count(FREEZE_EMBEDDINGS, "ELMO_LSTM") > 0 == false then true else false;

local GLOVE_TRAINABLE = if std.count(FREEZE_EMBEDDINGS, "GLOVE") > 0 == false then true  else false;

local ELMO_LSTM_TOKEN_INDEXER = if std.count(EMBEDDINGS, "ELMO_LSTM") > 0 then ELMO_LSTM_FIELDS(ELMO_LSTM_TRAINABLE)['elmo_lstm_indexer'] else {};

local GLOVE_TOKEN_INDEXER = if std.count(EMBEDDINGS, "GLOVE") > 0 then GLOVE_FIELDS(GLOVE_TRAINABLE)['glove_indexer'] else {};

local TOKEN_INDEXERS = ELMO_LSTM_TOKEN_INDEXER + GLOVE_TOKEN_INDEXER;

local ELMO_LSTM_TOKEN_EMBEDDER = if std.count(EMBEDDINGS, "ELMO_LSTM") > 0 then ELMO_LSTM_FIELDS(ELMO_LSTM_TRAINABLE)['elmo_lstm_embedder'] else null;
local GLOVE_TOKEN_EMBEDDER = if std.count(EMBEDDINGS, "GLOVE") > 0 then GLOVE_FIELDS(GLOVE_TRAINABLE)['glove_embedder'] else {};

local ELMO_LSTM_EMBEDDING_DIM = if std.count(EMBEDDINGS, "ELMO_LSTM") > 0 then ELMO_LSTM_FIELDS(ELMO_LSTM_TRAINABLE)['embedding_dim'] else 0;
local GLOVE_EMBEDDING_DIM = if std.count(EMBEDDINGS, "GLOVE") > 0 then GLOVE_FIELDS(GLOVE_TRAINABLE)['embedding_dim'] else 0;

local EMBEDDING_DIM = ELMO_LSTM_EMBEDDING_DIM + GLOVE_EMBEDDING_DIM;



{
  "numpy_seed": std.extVar("SEED"),
  "pytorch_seed": std.extVar("SEED"),
  "random_seed": std.extVar("SEED"),
  // Slightly modified version of the bi-attentative classification model with ELMo
  // from "Deep contextualized word representations" (http://www.aclweb.org/anthology/N18-1202),
  // trained on 5-class Stanford Sentiment Treebank.
  // There is a trained model available at https://s3-us-west-2.amazonaws.com/allennlp/models/sst-5-elmo-biattentive-classification-network-2018.09.04.tar.gz
  // with test accuracy of 54.7%.
  "dataset_reader":{
    "type": "sst_tokens",
    "use_subtrees": true,
    "granularity": "2-class",
    "token_indexers": TOKEN_INDEXERS
  },
  "validation_dataset_reader":{
    "type": "sst_tokens",
    "use_subtrees": false,
    "granularity": "2-class",
    "token_indexers": TOKEN_INDEXERS
  },

  "train_data_path": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/sst/train.txt",
  "validation_data_path": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/sst/dev.txt",
  "test_data_path": if EVALUATE_ON_TEST then "https://s3-us-west-2.amazonaws.com/allennlp/datasets/sst/test.txt" else null,
  "evaluate_on_test": EVALUATE_ON_TEST,
  "model": {
    "type": "bcn_modified",
    # The BCN model will consume the arrays generated by the ELMo token_indexer
    # independently of the text_field_embedder, so we do not include the elmo key here.
    "text_field_embedder": {
      "token_embedders": GLOVE_TOKEN_EMBEDDER
    },
    "embedding_dropout": std.extVar("EMBEDDING_DROPOUT"),
    "pre_encode_feedforward": {
        "input_dim": EMBEDDING_DIM,
        "num_layers": std.parseInt(std.extVar("PRE_ENCODE_FEEDFORWARD_LAYERS")),
        "hidden_dims": std.makeArray(std.parseInt(std.extVar("PRE_ENCODE_FEEDFORWARD_LAYERS")), function(i) std.parseInt(std.extVar("PRE_ENCODE_FEEDFORWARD_HIDDEN_DIMS"))),
        "activations": std.extVar("PRE_ENCODE_FEEDFORWARD_ACTIVATION"),
        "dropout": std.extVar("PRE_ENCODE_FEEDFORWARD_DROPOUT")
    },
    "encoder": {
      "type": "lstm",
      "input_size": std.parseInt(std.extVar("PRE_ENCODE_FEEDFORWARD_HIDDEN_DIMS")),
      "hidden_size": std.parseInt(std.extVar("ENCODER_HIDDEN_SIZE")),
      "num_layers": std.parseInt(std.extVar("ENCODER_NUM_LAYERS")),
      "bidirectional": true
    },
    "integrator": {
      "type": "lstm",
      "input_size": std.parseInt(std.extVar("ENCODER_HIDDEN_SIZE")) * 2 * 3,
      "hidden_size": std.parseInt(std.extVar("INTEGRATOR_HIDDEN_SIZE")),
      "num_layers": std.parseInt(std.extVar("INTEGRATOR_NUM_LAYERS")),
      "bidirectional": true
    },
    "integrator_dropout": std.extVar("INTEGRATOR_DROPOUT"),
    "elmo": ELMO_LSTM_TOKEN_EMBEDDER["elmo"],
    "use_input_elmo": if std.count(EMBEDDINGS, "ELMO_LSTM") > 0 then true else false,
    "use_integrator_output_elmo": std.parseInt(std.extVar("USE_INTEGRATOR_OUTPUT_ELMO")) == 1,
    "output_layer": {
        "input_dim": if std.parseInt(std.extVar("USE_INTEGRATOR_OUTPUT_ELMO")) == 1 then ( std.parseInt(std.extVar("INTEGRATOR_HIDDEN_SIZE")) * 2 + 1024 ) * 4 else std.parseInt(std.extVar("INTEGRATOR_HIDDEN_SIZE")) * 4 * 2,
        "num_layers": std.parseInt(std.extVar("OUTPUT_NUM_LAYERS")),
        "output_dims": std.makeArray(std.parseInt(std.extVar("OUTPUT_NUM_LAYERS")), function(i) std.parseInt(std.extVar("OUTPUT_DIM"))),
        "pool_sizes": std.parseInt(std.extVar("POOL_SIZES")),
        "dropout": std.extVar("OUTPUT_DROPOUT")
    }
  },
  "iterator": {
    "type": "bucket",
    "sorting_keys": [["tokens", "num_tokens"]],
    "batch_size" : BATCH_SIZE
  },
  "trainer": {
    "num_epochs": NUM_EPOCHS,
    "patience": 10,
    "grad_norm": GRAD_NORM,
    "validation_metric": "+accuracy",
    "cuda_device": CUDA_DEVICE,
    "optimizer": {
      "type": "adam",
      "lr": LEARNING_RATE
    },
    "learning_rate_scheduler": {
        "type": "reduce_on_plateau",
        "factor": 0.5,
        "patience": 2
      },
      "num_serialized_models_to_keep": 1
  }
}